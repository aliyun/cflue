(English|[ç®€ä½“ä¸­æ–‡](./README_zh.md))
# [Benchmarking Large Language Models on CFLUE - A Chinese Financial Language Understanding Evaluation Dataset](https://arxiv.org/abs/2405.10542) 
![index.jpg](./resources/index.png)
## CFLUE Version 1.0 â€” Chinese Financial Language Understanding Evaluation Dataset in the Financial Domain
Alibaba Cloud, in collaboration with Soochow University, has introduced CFLUE (Chinese Financial Language Understanding Evaluation), a novel and comprehensive benchmark designed to assess the understanding and processing capabilities of large language models within the context of Chinese financial language.

CFLUE evaluates the performance of language models through two main dimensionsâ€”Knowledge Assessment and Application Assessment.

- The Knowledge Assessment component consists of over 38,000 multiple-choice questions selected from 15 different types of financial qualification simulation exams, aimed at testing the language models' ability to predict answers and reason. Each question is accompanied by explanations, which aids in a thorough evaluation of the models' reasoning processes.

- The Application Assessment component provides over 16,000 instances covering five classic NLP tasks including text classification, machine translation, relation extraction, reading comprehension, and text generation. These instances are derived from existing shared tasks or annotated real data by professionals.

Overall, CFLUE offers multi-faceted insights for understanding and enhancing the capabilities of LLMs in the Chinese financial domain, and calls for more comprehensive and meticulous assessment of these models through CFLUE. The research team hopes that CFLUE will not only facilitate in-depth understanding of existing models but also drive new strides in the development of language models in the Chinese financial domain.

Currently, the CFLUE V1.0 evaluation dataset is available to the public, with plans to continuously update versions and introduce an integrated platform-based evaluation service in the future. This aims to provide a comprehensive, one-stop evaluation solution for the entire industry.

![CFLUE3.jpg](./resources/cflue.jpeg)
## Changelog
- **[2025.01.06]** Our entire dataset is now open-sourced and available for download! [Download here](https://www.modelscope.cn/datasets/tongyi_dianjin/CFLUE) ğŸš€ğŸš€ğŸš€
- **[2024.05.16]** Our paper "Benchmarking Large Language Models on CFLUE - A Chinese Financial Language Understanding Evaluation Dataset" has been officially accepted by ACL-2024! ğŸš€ğŸš€ğŸš€
- **[2024.03.06]** CFLUE has now open-sourced the development set for "Knowledge Assessment" and data samples for "Application Assessment" ğŸš€ğŸš€ğŸš€ï¼›
## Table of Contents

- [Leaderboard](#leaderboard)
- [Data](#data)
- [Quick Start](#quick-start)
- [How to Submit](#how-to-submit)
- [TODO](#todo)
- [Licenses](#licenses)
- [Citations](#citations)
## Leaderboard
Below, we list the zero-shot performance of the models we evaluated in the initial version. We have conducted a comprehensive assessment of various large language models on CFLUE, including OpenAI's GPT-4 and GPT-4-turbo, as well as several models for general and financial domains. The results show that GPT-4 and GPT-4-turbo significantly outperform other models in answer prediction for Knowledge Assessment, with accuracy exceeding 60%, demonstrating their leading position in the field but also implying significant room for improvement in other language models. In Application Assessment, although these two models perform well overall, their advantage is somewhat reduced compared to some models specifically designed for Chinese. The study also found that current LLMs in the financial field, such as FinGPT V3, DISC-FinLLM, and Tongyi-Finance, perform poorly in zero-sample tests, indicating that these modelsâ€™ coverage of financial knowledge needs to be strengthened. On the other hand, lightweight LLMs perform well after supervised fine-tuning, such as ChatGLM3-6B, Qwen-7B, and Baichuan2-7B, whose performance in some tasks is comparable to the larger-parameter ChatGPT.

### Knowledge Assessment
| **Model**          | **Acc**    | **Weighted-F1** | **BLEU-1** | **BLEU-4** | **ROUGE-1** | **ROUGE-2** | **ROUGE-L** |
|--------------------|------------| --- | --- | --- | --- | --- | --- |
| Qwen-72B           | 72.8Â±0.23  | 73.04Â±0.23 | 45.78Â±0.39 | 26.76Â±0.21 | 50.78Â±0.15 | 31.48Â±0.13 | 45.28Â±0.15 |
| GPT-4 | 60.87Â±0.11 | 60.82Â±0.1 | 37.58+0.18 | 17.26Â±0.09 | 44.5Â±0.12 | 22.42Â±0.08 | 32.59Â±0.11 |
| GPT-4-turbo | 60.61Â±0.21 | 60.31Â±0.19 | 30.66Â±0.22 | 10.61Â±0.13 | 40.28Â±0.2 | 17.23Â±0.15 | 28.62Â±0.19 |
| Qwen-14B           | 53.82Â±0.23 | 54.23Â±0.27 | 40.05Â±0.34 | 21.56Â±0.25 | 47.61Â±0.11 | 27.27Â±0.1 | 41.45Â±0.12 |
| Tongyi-Finance-14B | 47.21Â±0.11 | 47.07Â±0.16 | 38.32Â±0.11 | 19.24Â±0.05 | 44.35Â±0.07 | 23.55Â±0.06 | 38.1Â±0.1 |
| Qwen-7B-sft        | 48.61Â±0.58 | 48.59Â±0.6 | 37.7Â±1.94 | 20.74Â±1.14 | 47.62Â±0.19 | 27.73Â±0.17 | 42.41Â±0.15 |
| Qwen-7B            | 43.63Â±0.37 | 43.25Â±0.41 | 42.03Â±0.32 | 17.85Â±0.29 | 39.87Â±0.26 | 22.11Â±0.21 | 35.06Â±0.28 |
| ChatGPT            | 43.35Â±0.6  | 42.96Â±0.7 | 41.67Â±0.76 | 20.46Â±0.51 | 47.37Â±0.19 | 25.29Â±0.18 | 35.41Â±0.13 |
| ChatGLM3-6B-sft    | 42.43Â±0.24 | 41.93Â±0.27 | 12.96Â±1.39 | 6.64Â±0.76 | 43.06Â±0.3 | 24.08Â±0.3 | 38.17Â±0.29 |
| Baichuan2-13B      | 41.5Â±0.29  | 40.87Â±0.29 | 28.64Â±0.57 | 14.16Â±0.28 | 42.04Â±0.06 | 22.36Â±0.1 | 36.51Â±0.05 |
| ChatGLM3-6B        | 40.78Â±0.33 | 41.37Â±0.33 | 34.7Â±0.47 | 16.74Â±0.23 | 43.74Â±0.08 | 22.92Â±0.09 | 37.68Â±0.04 |
| Qwen-1.8B          | 38.68Â±0.26 | 38.53Â±0.26 | 40.25Â±0.12 | 19.01Â±0.08 | 42.43Â±0.11 | 23.08Â±0.09 | 37.17Â±0.13 |
| Baichuan2-7B       | 32.31Â±0.14 | 28.77Â±0.19 | 21.71Â±1.36 | 0.17Â±0.08 | 7.54Â±0.12 | 3.23Â±0.09 | 6.9Â±0.12 |
| Vicuna_v1.5        | 31.14Â±0.37 | 30.92Â±0.35 | 29.6Â±0.21 | 12.92Â±0.16 | 40.68Â±0.11 | 19.32Â±0.11 | 34.27Â±0.07 |
| LLaMA2-7B-sft      | 27.07Â±0.65 | 26.93Â±0.65 | 36.7Â±1.6 | 18.56Â±0.22 | 43.29Â±0.19 | 23.72Â±0.16 | 38.22Â±0.16 |
| LLaMA2-7B          | 18.79Â±0.25 | 15.54Â±0.21 | 13.11Â±0.11 | 5.49Â±0.07 | 22.02Â±0.19 | 9.72Â±0.14 | 19.06Â±0.2 |
| LLaMA2-70B         | 17.66Â±0.39 | 10.34Â±0.31 | 9.46Â±0.16 | 3.93Â±0.1 | 17.77Â±0.17 | 7.65Â±0.16 | 15.48Â±0.18 |

### Application Assessment
| **Model/Task**      | **Fin_TC** | **Fin_MT en2zh** | -          | **Fin_MT zh2en** | -          | **Fin_RE** | **Fin_RC** | **Fin_TG** | **Avg.** |
|---------------------| --- | --- |------------| --- |------------| --- | --- | --- | --- |
|                     | **ACC** | **BLEU-4** | **COMET**  | **BLEU-4** | **COMET**  | **F1** | **ROUGE-L** | **ROUGE-L** | - |
| GPT-4               | 61.23Â±0.03 | 21.92Â±0.03 | 78.32Â±0.09 | 21.05Â±0.02 | 87.20Â±0.13 | 53.45Â±0.09 | 46.34Â±0.06 | 27.55Â±0.05 | 49.63 |
| GPT-4-turbo         | 60.36Â±0.10 | 22.81Â±0.08 | 79.89Â±0.12 | 19.90Â±0.04 | 87.16Â±0.20 | 53.81Â±0.29 | 44.34Â±0.13 | 24.22Â±0.09 | 49.06 |
| Qwen-72B            | 51.06Â±0.20 | 22.08Â±0.07 | 79.20Â±0.03 | 23.89Â±0.03 | 87.21Â±0.06 | 49.21Â±0.11 | 43.33Â±0.05 | 30.52Â±0.02 | 48.31 |
| ChatGPT             | 52.42Â±0.16 | 21.20Â±0.12 | 78.21Â±0.11 | 19.65Â±0.08 | 86.82Â±0.11 | 52.30Â±0.19 | 47.43Â±0.11 | 26.76Â±0.06 | 48.10 |
| Qwen-14B-Chat       | 39.87Â±0.26 | 19.80Â±0.11 | 74.99Â±0.09 | 22.56Â±0.06 | 84.81Â±0.11 | 36.15Â±0.12 | 45.20Â±0.09 | 30.11Â±0.08 | 44.18 |
| Tongyi-Finance-14B  | 29.91Â±0.04 | 18.98Â±7.63 | 73.84Â±0.07 | 22.41Â±1.87 | 84.61Â±0.07 | 33.32Â±0.16 | 45.00Â±0.04 | 28.85Â±0.02 | 42.12 |
| Qwen-7B-Chat        | 26.07Â±0.62 | 18.10Â±0.08 | 72.53Â±0.13 | 19.27Â±0.04 | 82.69Â±0.11 | 35.15Â±0.38 | 44.36Â±0.05 | 28.00Â±0.09 | 40.77 |
| Baichuan2-13B-Chat  | 15.06Â±0.10 | 19.86Â±0.07 | 74.44Â±0.06 | 19.11Â±0.11 | 84.15Â±0.05 | 31.77Â±0.10 | 43.45Â±0.11 | 28.65Â±0.00 | 39.56 |
| Qwen-1.8B-Chat      | 23.90Â±0.41 | 15.22Â±1.53 | 66.79Â±0.10 | 14.04Â±5.87 | 72.63Â±0.21 | 23.97Â±0.10 | 43.78Â±0.07 | 26.41Â±0.07 | 35.84 |
| DISC-FinLLM-13B     | 23.24Â±0.06 | 15.50Â±0.13 | 70.95Â±0.12 | 4.46Â±0.05 | 80.63Â±0.14 | 32.11Â±0.29 | 43.32Â±0.08 | 24.16Â±0.10 | 36.80 |
| Chatglm3-6b         | 27.65Â±0.01 | 14.94Â±0.07 | 62.40Â±0.14 | 16.30Â±0.63 | 78.26Â±0.16 | 23.33Â±0.20 | 43.08Â±0.10 | 26.52Â±0.13 | 36.56 |
| vicuna-13B-v1.5-16k | 30.99Â±0.22 | 15.10Â±0.14 | 64.46Â±0.16 | 17.79Â±0.07 | 82.83Â±0.09 | 34.23Â±0.05 | 43.61Â±0.09 | 26.55Â±0.01 | 35.49 |
| Baichuan2-7B-Chat   | 18.91Â±0.25 | 18.78Â±0.53 | 50.85Â±0.11 | 18.11Â±0.11 | 52.20Â±0.07 | 23.29Â±0.11 | 24.86Â±0.07 | 15.46Â±0.12 | 32.49 |
| FinGPT V3-6B        | 19.10Â±0.03 | 13.90Â±0.12 | 60.64Â±0.21 | 13.63Â±0.08 | 73.48Â±0.26 | 19.16Â±0.24 | 39.75Â±0.12 | 17.33Â±0.05 | 32.12 |
| LLama2-70B          | 16.67Â±0.50 | 3.05Â±0.06 | 43.19Â±0.35 | 4.86Â±0.02 | 40.59Â±0.16 | 26.94Â±0.28 | 7.07Â±0.10 | 6.14Â±0.15 | 18.56 |
| Llama-2-7b-chat     | 4.01Â±0.04 | 1.59Â±0.05 | 28.34Â±0.14 | 3.37Â±0.06 | 34.68Â±0.18 | 21.48Â±0.25 | 4.19Â±0.03 | 1.09Â±0.01 | 12.34 |


## Data
The CFLUE evaluation [data](./data) in the data directory contains two subdirectories: [knowledge](./data/knowledge) and [application](./data/application). The former includes evaluation data for financial applications, while the latter contains sample evaluation data for financial applications.

**Financial Knowledge Evaluation Data**

| **ç§‘ç›®** | **Subject** |
| --- | --- |
| åŸºé‡‘ä»ä¸šèµ„æ ¼ | Asset Management Association of China |
| é‡‘èç†è´¢å¸ˆ | Associate Financial Planner |
| ä¼šè®¡ä»ä¸šèµ„æ ¼ | Certificate of Accounting Professional |
| é“¶è¡Œä¸­çº§èµ„æ ¼ | Certification of China Banking Professional (Intermediate) |
| é“¶è¡Œåˆçº§èµ„æ ¼ | Certification of China Banking Professional (Preliminary) |
| æœŸè´§ä»ä¸šèµ„æ ¼ | Certificate of Futures Qualification |
| è¯åˆ¸ä»ä¸šèµ„æ ¼ | Certification of Securities Professional |
| ä¸­å›½ç²¾ç®—å¸ˆ | Certified China Actuary |
| æ³¨å†Œä¼šè®¡å¸ˆ | Certified Public Accountant |
| ä¿é™©ä»ä¸šèµ„æ ¼ | China Insurance Certification & Education |
| åå‡è´§å¸è€ƒè¯• | Counterfeit Currency Detection Exam |
| é»„é‡‘ä»ä¸šèµ„æ ¼ | Gold Trading Qualification Certificate |
| ä¸­çº§ç»æµå¸ˆ | Intermediate Economics Professional Qualification |
| åˆçº§ç»æµå¸ˆ | Junior Economics Professional Qualification |
| è¯åˆ¸ä¸“é¡¹è€ƒè¯• | Securities Special Examination |

Here is a sample data entry for financial knowledge:
```python
{
    "åç§°":"è¯åˆ¸ä¸“é¡¹è€ƒè¯•",
    "task":"å•é¡¹é€‰æ‹©é¢˜",
    "question":"ç”¨å¤åˆ©è®¡ç®—ç¬¬næœŸç»ˆå€¼çš„å…¬å¼ä¸ºï¼ˆï¼‰ã€‚",
    "choices":"{'A': 'FV=PVÃ—(1+IÃ—n)', 'B': 'PV=FVÃ—(1+IÃ—n)', 'C': 'FV=PVÃ—(1+I)^n', 'D': 'PV=FVÃ—(1+I)^n'}",
    "answer":"C",
    "analysis":"å¤åˆ©æ˜¯è®¡ç®—åˆ©æ¯çš„å¦ä¸€ç§æ–¹æ³•ã€‚æŒ‰ç…§è¿™ç§æ–¹æ³•ï¼Œæ¯ç»è¿‡ä¸€ä¸ªè®¡æ¯æœŸï¼Œè¦å°†æ‰€ç”Ÿåˆ©æ¯åŠ å…¥æœ¬é‡‘å†è®¡åˆ©æ¯ã€‚å› æ­¤ï¼Œå¤åˆ©ç»ˆå€¼è®¡ç®—å…¬å¼ä¸ºï¼šFV=PVÃ—(1+I)^nã€‚"
}
```
Because financial knowledge includes three different types of question formatsâ€”single-choice, multiple-choice, and true/falseâ€”CFLUE utilizes corresponding prompt templates. By executing [utils/format_example.py](./utils/format_example.py), one can quickly load the data and build the final model input. Below are examples of prompt templates for single-choice and multiple-choice questions:


- Single-Choice Question Prompt Template
```python
å‡è®¾ä½ æ˜¯ä¸€ä½é‡‘èè¡Œä¸šä¸“å®¶ï¼Œè¯·å›ç­”ä¸‹åˆ—é—®é¢˜ã€‚
æ³¨æ„ï¼šé¢˜ç›®æ˜¯å•é€‰é¢˜ï¼Œåªéœ€è¦è¿”å›ä¸€ä¸ªæœ€åˆé€‚çš„é€‰é¡¹ï¼Œè‹¥æœ‰å¤šä¸ªåˆé€‚çš„ç­”æ¡ˆï¼Œåªè¿”å›æœ€å‡†ç¡®çš„å³å¯ã€‚
æ³¨æ„ï¼šç»“æœåªè¾“å‡ºä¸¤è¡Œï¼Œç¬¬ä¸€è¡Œåªéœ€è¦è¿”å›ç­”æ¡ˆçš„è‹±æ–‡é€‰é¡¹(æ³¨æ„åªéœ€è¦è¿”å›ä¸€ä¸ªæœ€åˆé€‚çš„ç­”æ¡ˆ)ï¼Œç¬¬äºŒè¡Œè¿›è¡Œç®€è¦çš„è§£æï¼Œè¾“å‡ºæ ¼å¼é™åˆ¶ä¸ºï¼šâ€œç­”æ¡ˆï¼šâ€ï¼Œâ€œè§£æï¼šâ€ã€‚

{question}
{choices}
```

- Multiple-Choice Question Prompt Template
```python
å‡è®¾ä½ æ˜¯ä¸€ä½é‡‘èè¡Œä¸šä¸“å®¶ï¼Œè¯·å›ç­”ä¸‹åˆ—é—®é¢˜ã€‚
æ³¨æ„ï¼šé¢˜ç›®æ˜¯å¤šé€‰é¢˜ï¼Œå¯èƒ½å­˜åœ¨å¤šä¸ªæ­£ç¡®çš„ç­”æ¡ˆã€‚
æ³¨æ„ï¼šç»“æœåªè¾“å‡ºä¸¤è¡Œï¼Œç¬¬ä¸€è¡Œåªéœ€è¦è¿”å›ç­”æ¡ˆçš„è‹±æ–‡é€‰é¡¹ï¼Œç¬¬äºŒè¡Œè¿›è¡Œç®€è¦çš„è§£é‡Šã€‚è¾“å‡ºæ ¼å¼é™åˆ¶ä¸ºï¼šâ€œç­”æ¡ˆï¼šâ€ï¼Œâ€œè§£æï¼šâ€ã€‚

{question}
{choices}
```
**Financial Application Evaluation Data**

<table>
    <tr>
        <th>Task</th>
        <th>Sub Task</th>
    </tr>
    <tr>
        <td rowspan="6">é‡‘èæ–‡æœ¬åˆ†ç±»/Fin_TC (Financial Text Classification)</td>
        <td>ESGåˆ†ç±»/ESG Classification (14)</td>
    </tr>
    <tr>
        <td>ESGæƒ…æ„Ÿåˆ†æ/ESG Sentiment Analysis (3)</td>
    </tr>
    <tr>
        <td>é‡‘èè¡Œä¸šåˆ†ç±»/Industry Classification (68)</td>
    </tr>
    <tr>
        <td>é‡‘èä¼šè®®ä¸šåŠ¡åˆ†ç±»/Conference Services Classification (3)</td>
    </tr>
    <tr>
        <td>é‡‘èäº‹ä»¶åˆ†ç±»/Event Classification (27)</td>
    </tr>
    <tr>
        <td>é“¶è¡Œå®¢æœå¯¹è¯æ„å›¾åˆ†ç±»/Banking Customer Service Intent Classification (77)</td>
    </tr>
    <tr>
        <td rowspan="4">é‡‘èæ–‡æœ¬æŠ½å–/Fin_RE (Financial Text Extraction)</td>
        <td>è¡Œä¸šæƒ…æ„Ÿä¿¡æ¯æŠ½å–/Industry Sentiment Information Extraction</td>
    </tr>
    <tr>
        <td>é‡‘èäº‹ä»¶æŠ½å–/Financial Event Extraction</td>
    </tr>
    <tr>
        <td>é‡‘èäº‹ä»¶å› æœå…³ç³»æŠ½å–/Financial Event Causality Extraction</td>
    </tr>
    <tr>
        <td>é‡‘èäº‹ä»¶ä¸»ä½“æŠ½å–/Financial Event Entity Extraction</td>
    </tr>
    <tr>
        <td>é‡‘èé˜…è¯»ç†è§£/Fin_RC (Financial Reading Comprehension)</td>
        <td> --- </td>
    </tr>
    <tr>
        <td rowspan="2">é‡‘èç¿»è¯‘/Fin_MT (Financial Translation)</td>
        <td>é‡‘èè‹±ä¸­ç¿»è¯‘/English-Chinese Translation</td>
    </tr>
    <tr>
        <td>é‡‘èä¸­è‹±ç¿»è¯‘/Chinese-English Translation</td>
    </tr>
    <tr>
        <td rowspan="5">é‡‘èæ–‡æœ¬ç”Ÿæˆ/Fin_TG (Financial Text Generation)</td>
        <td>é‡‘èæœ¯è¯­è§£é‡Š/Financial Term Explanation</td>
    </tr>
    <tr>
        <td>ä¼šè®®å†…å®¹æ‘˜è¦/Meeting Content Summary</td>
    </tr>
    <tr>
        <td>å®¢æœå¯¹è¯æ‘˜è¦/Customer Service Dialogue Summary</td>
    </tr>
    <tr>
        <td>èµ„è®¯æ ‡é¢˜ç”Ÿæˆ/News Headline Generation</td>
    </tr>
    <tr>
        <td>ç ”æŠ¥æ ‡é¢˜ç”Ÿæˆ/Research Report Headline Generation</td>
    </tr>
</table>


For example, in financial text classification, the data samples are as follows:

```python
{
    "task":"é‡‘èæ–‡æœ¬åˆ†ç±»",
    "sub_task":"ESGåˆ†ç±»",
    "instruction":"ä½ æ˜¯ä¸€ä¸ªé‡‘èè¡Œä¸šçš„æ–‡æœ¬åˆ†ç±»ä¸“å®¶ï¼Œè¯·å°†ä¸‹é¢çš„ESGå…¬å‘Šè¿›è¡Œåˆ†ç±»ï¼Œç±»åˆ«åŒ…æ‹¬ï¼š['é¡¾å®¢å¥åº·ä¸å®‰å…¨', 'ç‰©æ–™', 'åè…è´¥è¡Œä¸º', 'éè™šå‡è¥é”€', 'å¤šæ ·æ€§ä¸æœºä¼šå¹³ç­‰', 'ä¾æ³•åˆè§„çº³ç¨', 'éç›´æ¥ç»æµå½±å“', 'å…¬å…±æ”¿ç­–', 'æ°´èµ„æºä¸æ±¡æ°´å¤„ç†', 'èŒä¸šåŸ¹è®­åŠæ•™è‚²', 'ç»æµç»©æ•ˆ', 'åä¸æ­£å½“ç«äº‰', 'å®‰å…¨ç®¡ç†å®è·µ', 'èƒ½æº', 'å¸‚åœºå æœ‰ç‡', 'æ’æ”¾']ã€‚è¯·ä»…è¾“å‡ºåˆ†ç±»ç»“æœï¼Œä¸è¦åšå¤šä½™çš„è§£é‡Šï¼Œç­”æ¡ˆæœ‰ä¸”åªæœ‰ä¸€ä¸ªã€‚\nå…¬å‘Šï¼šæœ¬å…¬å¸åŠè‘£äº‹ä¼šå…¨ä½“æˆå‘˜ä¿è¯ä¿¡æ¯æŠ«éœ²çš„å†…å®¹çœŸå®ã€å‡†ç¡®ã€å®Œæ•´ï¼Œæ²¡æœ‰è™šå‡è®°è½½ã€è¯¯å¯¼æ€§é™ˆè¿°æˆ–é‡å¤§é—æ¼ã€‚ 2010å¹´3æœˆ16æ—¥ï¼Œå‰æ—å…‰åæ§è‚¡é›†å›¢è‚¡ä»½æœ‰é™å…¬å¸ï¼ˆä»¥ä¸‹ç®€ç§°\"å…¬å¸\"ï¼‰æ¥åˆ°å…¬å¸ç¬¬ä¸€å¤§è‚¡ä¸œæ±Ÿè‹å¼€å…ƒèµ„äº§ç®¡ç†æœ‰é™å…¬å¸ï¼ˆä»¥ä¸‹ç®€ç§°\"å¼€å…ƒèµ„äº§\"ï¼‰è½¬å‘çš„æ±Ÿè‹çœäººæ°‘æ”¿åºœå›½æœ‰èµ„äº§ç›‘ç£ç®¡ç†å§”å‘˜ä¼šã€Šå…³äºåŒæ„å…¬å¼€è½¬è®©æ±Ÿè‹å¼€å…ƒèµ„äº§ç®¡ç†å…¬å¸9%è‚¡æƒçš„æ‰¹å¤ã€‹ï¼ˆè‹å›½èµ„å¤[2010]27å·ï¼‰ã€‚æ‰¹å¤å†…å®¹å¦‚ä¸‹ï¼š åŒæ„æ±Ÿè‹å¼€å…ƒå›½é™…é›†å›¢æœ‰é™å…¬å¸ï¼ˆä»¥ä¸‹ç®€ç§°\"å¼€å…ƒé›†å›¢\"ï¼‰å°†æ‰€æŒå¼€å…ƒèµ„äº§9%çš„è‚¡æƒï¼Œé€šè¿‡äº§æƒäº¤æ˜“æœºæ„è¿›è¡Œå…¬å¼€è½¬è®©ã€‚æ ¹æ®å›½å®¶å…³äºä¼ä¸šå›½æœ‰äº§æƒè½¬è®©çš„æœ‰å…³è§„å®šï¼ŒåŠç†èµ„äº§è¯„ä¼°ç›¸å…³æ‰‹ç»­ï¼Œåˆç†åˆ¶å®šäº§æƒè½¬è®©å…¬å‘Šä¸­å—è®©æ–¹åº”å…·å¤‡çš„åˆç†æ€§ã€å…¬å¹³æ€§èµ„æ ¼æ¡ä»¶ï¼Œç¡®ä¿äº§æƒè½¬è®©çš„å…¬å¼€ä¸è§„èŒƒã€‚å¹¶æŒ‰ç…§ã€Šå›½æœ‰è‚¡ä¸œè½¬è®©æ‰€æŒä¸Šå¸‚å…¬å¸è‚¡ä»½ç®¡ç†æš‚è¡ŒåŠæ³•ã€‹ï¼ˆå›½èµ„å§”ã€è¯ç›‘ä¼šä»¤ç¬¬19å·ï¼‰è¦æ±‚ï¼ŒåŠæ—¶å±¥è¡Œä¿¡æ¯æŠ«éœ²ç­‰ç›¸å…³ä¹‰åŠ¡ï¼Œåœ¨åŠç†äº§æƒè½¬è®©é‰´è¯å‰ï¼ŒæŒ‰è§„å®šç¨‹åºæŠ¥å›½åŠ¡é™¢å›½èµ„å§”å®¡æ ¸æ‰¹å‡†ã€‚ å¼€å…ƒé›†å›¢é¢„è®¡å°†åœ¨è¿‘æ—¥äºæ±Ÿè‹çœäº§æƒäº¤æ˜“æ‰€è¿›è¡Œå…¬å¼€æŒ‚ç‰Œè½¬è®©å¼€å…ƒèµ„äº§9%è‚¡æƒã€‚å¼€å…ƒé›†å›¢æŒæœ‰å¼€å…ƒèµ„äº§51%è‚¡æƒï¼Œä¸Šæµ·æ³°æ³“æŠ•èµ„ç®¡ç†æœ‰é™å…¬å¸æŒæœ‰å¼€å…ƒèµ„äº§49%è‚¡æƒï¼Œè‹¥æ­¤æ¬¡å…¬å¼€è½¬è®©æˆåŠŸï¼Œå¼€å…ƒé›†å›¢æŒæœ‰å¼€å…ƒèµ„äº§çš„è‚¡æƒå°†ä¸‹é™è‡³42%ï¼Œå°†å¯¼è‡´å¼€å…ƒèµ„äº§æ§è‚¡æƒå‘ç”Ÿå˜åŒ–ï¼Œè¿›è€Œå¯¼è‡´å…¬å¸å®é™…æ§åˆ¶äººå‘ç”Ÿå˜åŒ–ã€‚å…¬å¸å°†æ ¹æ®è¯¥äº‹é¡¹è¿›å±•æƒ…å†µå±¥è¡ŒæŒç»­ä¿¡æ¯æŠ«éœ²ä¹‰åŠ¡ã€‚ ç‰¹æ­¤å…¬å‘Šã€‚ å‰æ—å…‰åæ§è‚¡é›†å›¢è‚¡ä»½æœ‰é™å…¬å¸ è‘£äº‹ä¼š 2010å¹´3æœˆ16æ—¥\nåˆ†ç±»ç»“æœï¼š",
    "input":"",
    "output":"éç›´æ¥ç»æµå½±å“",
    "history":[]
}
```

Unlike financial knowledge, the 'instruction' field in financial application evaluation data refers to the model input field that has been assembled using the corresponding prompt, allowing for the immediate commencement of the evaluation task.

## Quick Start
```python
#!/bin/bash

model_name=""  # æ¨¡å‹åç§°
checkpoint_path=""  # æŒ‡å®šæ¨¡å‹checkpointè·¯å¾„
eval_type="knowledge"  # æŒ‡å®šé‡‘èçŸ¥è¯†è¯„æµ‹æˆ–é‡‘èåº”ç”¨è¯„æµ‹
save_result_dir="../results"

python cflue_main.py \
    --model_name ${model_name} \
    --checkpoint_path ${checkpoint_path} \
    --eval_type ${eval_type} \
    --save_result_dir ${save_result_dir}
```

## How to Submit
If you wish to participate in the evaluation of the custom large model on the Test set, you will first need to prepare a UTF-8 encoded JSON file and write it in the format of [submission_example.json](./submission_example.json). Then, send it as an attachment in an email formatted as below to [CFLUE@alibabacloud.com](mailto:CFLUE@alibabacloud.com) to apply.

```
Organization:
Contact Information:
Model Name:
Model Results Attachment ğŸ“
```

## TODO
- [x] Release the development set for the "Knowledge Assessment" section and data samples for the "Application Assessment";
- [x] Include zero-shot results;
- [x] **All data is fully open-sourced.**


## Licenses
![](https://img.shields.io/badge/License-MIT-blue.svg#id=wZ1Hr&originHeight=20&originWidth=82&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)
This project adheres to [MIT License](https://lbesson.mit-license.org/).

![](https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg#id=ZNe2m&originHeight=20&originWidth=158&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)
The CFLUE dataset follows [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-nc-sa/4.0/).


## Citations
If you use our dataset, please cite our paper.
```
@inproceedings{zhu2024cflue,
title={Benchmarking Large Language Models on CFLUE - A Chinese Financial Language Understanding Evaluation Dataset}, 
author={Jie Zhu, Junhui Li, Yalong Wen, Lifan Guo},
booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics(ACL-2024)},
year={2024}
}
```




